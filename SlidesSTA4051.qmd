---
title: "STA 4051 Nonparametric Statistics"
format:
  revealjs:
    theme: night
    slide-number: true
    chalkboard: 
      buttons: true
    footer: STA4051 - Nonparametric Statistics - Dr. Cohen
    width: 1200
    height: 900
---

## Welcome to STA 4051

This course covers an introduction to nonparametric statistics methods as follows:

-   Binomial Based Tests
-   Rank based Test
-   Contingency Tables
-   Goodness of fit Test



## What is Statistics?

-   Statistics deals with 4 subtopics:
    - Data Collection
    - Data Analysis
    - Interpretation of Results
    - Visualization
-   Descriptive vs Inferential
-   Parametric vs Nonparametric




## Nonparamertic Methods?

- Nonparametric methods require few assumptions about the underlying population (e.g. normal)
- Exact p-values for tests and exact coverage probabilities for confidence intervals.
- Nonparametric methods are relatively insensitive to outliers
- Nonparametric methods are only slightly less efficient than their normal theory competitors
- Nonparametric methods apply to data that are nominal, ordinal or ranks
- If a small sample size is used n=3.


## Definitions (review)
- **Population**: is the large body of data that is the target of our interest.
- **Sample**: is a subset of the population.
- **Sample space**: is the collection of all possible outcomes of an experiment.
- **Event**: is any subset of the sample space.

## What is Probability? (review)

- Intuitive: A probability is a measure of one's believe in the occurrence of an event.
- Axiomatic: Consider A is an event in the sample space S then:
    - $0 \leq P(A) \leq 1$
    - $P(S)=1$
    - The probability of the union of mutually exclusive events is the sum of the probabilities of the individual events.

## Random Variables? (review)

- Definition: A function that assigns real numbers to the points in a sample space.
- Discrete: values are finite or countably infinite
- Continuous: values in an interval
- Probability distributions:
    - *Discrete*: binomial($n$,$p$), geometric($p$), Poisson($\lambda$), etc.
    - *Continuous*: N($\mu$,$\sigma^2$), Gamma($\alpha$,$\beta$), etc.
    - *Sampling distributions*: $\chi^2_{df}$, $F_{u,v}$, $t_{n}$
  
  
## Expected Values? (review)


Let's $X \sim p(x)$ be a discrete r.v., then the expected value:

$$E(X) = \sum_{\forall x} x p(x)$$

Let's $X \sim f(x)$ be a continuous r.v., then the expected value:

$$E(X) = \int_{\forall x} x f(x) dx $$

- Variance: $Var(X) = E(X^2) - E(X)^2$
- Properties:
    - $E(a) = a$; $E(aX+b) = aE(X)+b$
    - $Var(a) = 0$; $V(aX+b) = a^2 Var(X)$



    
## Introduction to R/RStudio

- [**R**](https://www.r-project.org/) is a free software environment for statistical computing and graphics.

- [**RStudio**](https://www.rstudio.org) is an integrated development environment (IDE) for R and Python, with a console, syntax-highlighting editor, and other features.


- [**R Markdown**](https://rmarkdown.rstudio.com/) provides an authoring framework for data science. You can use a single R Markdown file to both save and execute code to generate high quality reports.

- [**Quarto**](https://quarto.org/) is an open-source scientific and technical publishing system.

- In this class, we will use the UWF RStudio Server. Log in using your **UWF account.** Go to [UWF RStudio Server](https://rstudio.hmcse.uwf.edu/)



## Quantiles

> The number $x_p$ ($0 \leq p \leq 1$) is called the $p^{th}$ quantile of a random variable $X$ if:
- Continuous: $P(X\leq x_p) = p$
- Discrete: $P(X < x_p) \leq p$ AND $P(X\leq x_p) \geq p$
    
We can think of quantiles as points that split the distribution (values of $X$) into equal intervals.

 **Examples**:
 
 - 2-quantile $\equiv$ Median (Q2=$x_{0.5}$)
 - 4-quantile $\equiv$ quartiles (Q1=$x_{0.25}$,Q2,Q3=${0.75}$)
 - 100-quantile $\equiv$ percentile ($x_{0.01},x_{0.02},\ldots,x_{0.99}$)
 - 10-quantiles $\equiv$ decile ($x_{0.1},x_{0.2},\ldots,x_{0.9}$)
 

## Quantiles 

Let's $X$ be a r.v. with PMF:

| x | 0 | 1 | 2 | 3 |
|---------|-----|------|------|------|
| P(X=x)      | 1/4 | 1/4| 1/3 |1/6|

What is $x_{0.75}$ the upper-quartile?

>Solution: Find $x_{0.75}$ that satisfies $P(X < x_{0.75}) \leq 0.75$ AND $P(X\leq x_{0.75}) \geq 0.75$; if more than one value satisfy this then the average of all values is the answer. ($x_{0.75}$=2)

What is $x_{0.5}$ the median?

>Solution: Find $x_{0.5}$ that satisfies $P(X < x_{0.5}) \leq 0.5$ AND $P(X\leq x_{0.5}) \geq 0.5$; if more than one value satisfy this then the average of all values is the answer.($x_{0.5}$=1.5)


## Quantiles 

Example 1:

```{r, echo=TRUE}
x = (1:12) #a vector from 1 to 12
x
median= quantile(x,p=c(0.5)) # Median of x
median
quartiles= quantile(x,p=c(0.25,0.5,0.75)) # quartiles of x
quartiles
```

Example 2:

```{r, echo=TRUE}
summary(mtcars$mpg)
```  
    
    
    

## Hypothesis Testing
    
>Hypothesis Testing is the process of inferring from a sample whether or not a given statement
about the population appears to be true.

- **Null hypothesis** - $H_0$: it is usually formulated for the express purpose of being rejected.
No differences. Example: "The process is in-control" in quality control.

- **Alternative Hypothesis** - $H_1$: $H_0$ is rejected. It is the statement that the experimenter would like to prove. There are differences. For example,
the alternative hypothesis could be "the quality of the product or service is unsatis-
factory" or \the process is out of control".
    
    
## Hypothesis Testing
    
- **The Test Statistic** is chosen to be sensitive to the difference between the null hypothesis and the alternative hypothesis.

- **Level of significance ($\alpha$)**: When the null hypothesis and the alternative hypothesis have been stated, and when the appropriate statistical test has been selected, the next step is to specify a level of significance $\alpha$ and a sample size. Since the level of significance goes into the determination of whether $H_0$ is or is not rejected, the requirement of objectivity demands that $\alpha$ be set in advance. This is also called Type I Error.

$$
\alpha = P(Reject~H_0~|~ H_0~is~True)
$$



## Hypothesis Testing
    
- **The Null distribution** is the distribution of the test statistic when $H_0$ is `TRUE`. This
defines the rejection region along with the level of significance.

- **The Power**: The probability of rejecting $H_0$ when it is in fact `FALSE`. 
$$Power = 1 - \beta = P(Rejecting~ H_0 | H_0 ~is~FALSE)$$
- **P-value** is the probability, computed assuming that $H_0$ is `TRUE`, that the test statistic
would take a value as extreme or more extreme than that actually observed.


- Accepting or Failing to Reject $H_0$ does not mean that the data prove the null hypothesis
to be true. Read the [ASA statement on p-values](https://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108?needAccess=true) 


  
    
## Binomial Test (Estimation of $p$)    

#### Binomial distribution
A binomial experiment has the following properties:

- The experiment consists of $n$ identical independent trials.
- Each trial results in one of two possible outcomes, $yes$ or $no$.
- The probability of success ($yes$) in each trial is the same, denoted as $p$.
- The `random variable` $Y$ is the number of successes ($yesS$) observed during the $n$ trials.


## Binomial Test (Estimation of $p$)  
#### Binomial distribution
Consider $Y \sim binom(n,p)$, then $Y$ can take on values $0, 1, 2, 3,\ldots, n$. The probability mass function is given by
\begin{align}
p(y)=&P(Y=y)=\binom{n}{y}p^y q^{n-y}, \\
 y=&0,1,2,\ldots,n ; \qquad 0\leq p \leq 1 ; \quad q=1-p
\end{align}
The expected value of $Y$ is:
\begin{align}E(Y)=np\end{align}
The Variance $Y$ is:
\begin{align}
V(Y)=npq
\end{align}
    
    
## Binomial Test (Estimation of $p$)  

There are some populations which are conceived as consisting of only two classes:

- member and nonmember, 
- married and single, 
- male and female. 

For such cases all the population observations will fall into either one or the other of the two classes. If we know that the proportion of cases in one class is $p$, sometimes we are interested to estimate/test the population proportion $p$.

## Binomial Test (Estimation of $p$)  

`1.DATA:`
The sample consists of outcomes of $n$ independent trails. Each trail has two outcomes (mutually exclusive= they cannot both occur at same time). Let consider $O_1$ to be $\#$ of observations in class1 and $O_2$ to be the $\#$ of observations in class2. We have $n=O_1+O_2$. 

`2.Assumptions:`
- $n$ observation are independent.
- Each trial has probability $p$ of resulting in the outcome $O_1$, where $p$ is the same for all $n$ trials.

`3.Test Statistic:`
Since we are interested in the probability of an outcome to be in class1, we will let the test statistic $T$ be `the number of times the outcomes is in class1`. Therefore, the test statistic follows the binomial distribution (null distribution) with $n$ and $\hat{p}$ (the hypothesized proportion) parameters.


## Binomial Test (Estimation of $p$)  

`4.Hypothesis: Two-tailed test`
$$ H_0: p=p^* $$ $$H_1: p \neq p^* $$  
- The rejection region is defined by $t_1$ and $t_2$ as follows:
$$ P( T \leq t_1) \approx \alpha/2 $$ $$ P( T \leq t_2) \approx 1- \alpha/2 $$

- `Decision`: IF $(T_{Observed} \leq t_1$ OR $T_{Obs}> t_2$)  then REJECT $H_0$

- `P-value`$= 2\times \min\{  P( Y \leq T_{Obs}), P(Y \geq T_{Obs}) \}$; If greater than 1 then $P-value=1$.

## Binomial Test (Estimation of $p$)  

`4.Hypothesis: Upper-tailed test`
$$ H_0: p \leq p^* $$ 
$$H_1: p > p^* $$ 
The rejection region is defined by $t$ as follows:

$$P( T \leq t_2) \approx 1-\alpha $$ 

- `Decision`: IF $T_{Obs} > t$ then REJECT $H_0$

- `P-value` $=P(Y \geq T_{Obs})$


## Binomial Test (Estimation of $p$)  

`4.Hypothesis: Lower-tailed test`
$$ H_0: p \geq p^* $$
$$H_1: p < p^* $$ 

- The rejection region is defined by $t$ as follows:
$$P( T \leq t_1) \approx \alpha $$ 

- `Decision`: IF $T_{Obs} \leq  t$ then REJECT $H_0$

- `P-value` $=P(Y \leq T_{Obs})$


## Binomial Test - Example

::: {.r-fit-text}

We would like to know whether the passing rate for a course is $75\%$. We have a sample of $n=15$ students taking the course and only 8 passed. Level of significance is $5\%$. Test the appropriate hypothesis as follows:

1. Define the null and alternative hypotheses <br>
`Answer:` $$ H_0: p=0.75 $$ $$H_1: p \neq 0.75 $$
2. Find the Test statistic observed and null distribution<br>
`Answer:` $T_{obs}=8$ and $T\sim bin(15,0.75)$
3. Determine critical values (rejection region) <br>
`Answer:` t_1=`r {qbinom(0.025,15,0.75)}` and t_2=`r {qbinom(0.975,15,0.75)}`
4. Find P-value<br>
`Answer:` p-value=`r {2*min(pbinom(8,15,0.75),pbinom(13,15,0.75,lower.tail=F))}`
5. Decision and Interpretation
<br>
`Answer:` Since P-value > 0.05 then Fail to Reject $H_0$.

:::

## Binomial Test - Example

`Solution`
```{r, echo=TRUE}
 binom.test(x = 8, # number of successes (#students pass)
 n = 15, # sample size
 p = 0.75, # hypothesized p
 alternative = 'two.sided', # two-tailed test
 conf.level = 0.95) # Confidence Interval
```

- `Interpretation`: Fail to Reject $H_0$. There is evidence to support that the data is compatible with the null hypothesis $\hat{p}=0.533$; $95\%CI (0.27,0.79)$. 



## Sign Test 

- The sign test is a binomial test with $p=0.5$. 

- It is useful for testing whether one random variable in a pair $(X, Y)$ tends to be larger than the other random variable in the pair.

- `Example`: Consider a clinical investigation to assess the effectiveness of a new drug designed to reduce repetitive behavior, we can compare time before and after taking the new drug.

- `This test can be use as alternative to the parametric t-paired test`.



## Sign Test 


`1.DATA:`
The data consist of observations on a bivariate random sample $(X_i, Y_i)$, where $n'$ is the number of the `pairs`. If the X's and Y's are independent, then the more powerful Mann-Whitney test is more appropriate.

Within each pair $(X_i, Y_i)$ a comparison is made and the pair is classified as:

- `"+"` if $X_i < Y_i$
- `"-"` if $X_i > Y_i$
- `"0"` if $X_i = Y_i$ (tie)


`2.Assumptions:`
- The bivariate random variables $(X_i, Y_i)$ are mutually independent .
- The measurement scale is at least ordinal.

## Sign Test 

`3.Test Statistic:`
The test statistic is defined as follows:

$$T = \text{Total number of +'s}$$

The null distribution is the binomial distribution with $p=0.5$ and $n=\text{the number of non-tied pairs}$.


## Sign Test

`4.Hypothesis: Two-tailed test`
$$ H_0:E(X_i) = E(Y_i) $$ 
$$H_1: E(X_i) \neq E(Y_i) $$
- The rejection region is defined by $t$ and $n-t$ where:
$$ P( Y \leq t) \approx \alpha/2 $$


- `Decision`: IF $(T_{Obs} \leq t$ or $T_{Obs} \geq n-t) $ REJECT $H_0$

- `P-value`$= 2\times \min\{  P( Y \leq T_{Obs}), P(Y \geq T_{Obs}) \}$


## Sign Test

`4.Hypothesis: Upper-tailed test`

$$ H_0:E(X_i) \geq E(Y_i) $$ 
$$H_1: E(X_i) < E(Y_i) $$
- The rejection region is defined by $n-t$ as follows:
$$P( Y \leq t) \approx \alpha $$
- `Decision`: IF $T_{Obs} \geq n-t$ REJECT $H_0$

- `P-value`$= P( Y \geq T_{Obs})$

## Sign Test

`4.Hypothesis: Lower-tailed test`

$$ H_0:E(X_i) \leq E(Y_i) $$ 
$$H_1: E(X_i) > E(Y_i) $$
- The rejection region is defined by $t$ where:
$$P( Y \leq t) \approx \alpha $$
- `Decision`: IF $T_{Obs} \leq t$ REJECT $H_0$

- `P-value`$= P( Y \leq T_{Obs})$



## Sign Test - Example

- 18 office workers. 
- We are interested in comparing the reaction time before lunch (RTBL) vs the reaction time after lunch (RTAL).
- 12 found their RTBL was shorter
- 1 could not detect a difference
- Is the RTAL significantly longer than RTBL?<br>
`Answer`

We have $n'=18$ pairs, each one for a worker:
$$ (RTBL,RTAL) = (X,Y)$$
- `"+"` if $RTBL < RTAL$ (12)
- `"-"` if $RTBL > RTAL$ (5)
- `"0"` if $RTBL = RTAL$ (tie) (1

then $n=17$


## Sign Test - Example

1. Define the null and alternative hypotheses <br>
`Answer:` $$ H_0: E(RTBL) = E(RTAL)$$ $$H_1: E(RTBL) < E(RTAL) $$
This is an upper-tailed test.
2. Find the Test statistic observed and null distribution<br>
`Answer:` $T_{obs}=12$ and $T\sim bin(17,0.5)$
3. Determine critical values (rejection region) <br>
`Answer:` `r {17-qbinom(0.05,17,0.5)}`
4. Find P-value<br>
`Answer:` p-value=`r {pbinom(11,17,0.5,lower.tail=F)}`
5. Decision and Interpretation
<br>
`Answer:` Since P-value < 0.05 then Reject $H_0$.




## Sign Test - Example

`Solution`
```{r, echo=TRUE}
 binom.test(x = 12,
 n = 17, 
 p = 0.5,
 alternative = 'g',
 conf.level = 0.95) # Confidence Interval
```

- `Interpretation`: Fail to Reject $H_0$. There is evidence to support that the data is compatible with the null hypothesis $\hat{p}=0.71$; $95\%CI (0.48,1)$. 




## Tolerance limits

- Confidence limits are limits within which we expect a given population parameter, such as the mean, to lie. 
- Tolerance limits are limits within which we expect a stated proportion of the population to lie with a certain probability. Formally:

$$
P(X_{(r)} \leq \text{a fraction q of the population} \leq X_{(n+m-1)}) \geq 1-\alpha
$$

## Tolerance limits

The tolerance limits can be use to find **A sample size $n$** needed to have at least $q$ proportion of the population between the tolerance limits with $1-\alpha$ probability.
    
$$ n \approx  \frac{1}{4} \chi^2_{1-\alpha;2(r+m)}  \frac{1+q}{1-q} + \frac{1}{2} (r+m-1)$$

where $\chi^2_{1-\alpha;2(r+m)}$ is the quantile of a chi-squared random variable.


## Tolerance limits

We can find The percent $q$ of the population that is within the tolerance limits, given n,  $1-\alpha$, $r$, and $m$:
    
$$q=\frac{4n-2(r+m-1)-\chi^2_{1-\alpha;2(r+m)}}{4n-2(r+m-1) + \chi^2_{1-\alpha;2(r+m)}}$$


## Tolerance limits - Example

Electric seat adjusters for a luxury car manufacturer wants to know what `range` of vertical adjustment  is needed to be $90\%$ sure that at least $80\%$ of population of potential buyers will be able to adjust their seat.

- Range: r=1, m=1
- 1-$\alpha$ = 0.9
- $q$ = 0.8

To answer the question we need to find $n$

```{r, echo=TRUE}
n= 0.25*qchisq(0.9,2*(1+1))*(1+0.8)/(1-0.8) + 0.5*(1+1-1)
n
```

Next, we need to randomly pick 18 people from the population of potential buyers and collect their adjustments. 

